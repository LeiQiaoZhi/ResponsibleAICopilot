{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ac4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste you API key below.\n",
    "# You can find your API key at https://platform.openai.com/account/api-keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56440c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Select features\n",
    "features = ['age', 'name', 'gender', 'address','phone_number']\n",
    "\n",
    "data = data[data['gender'] == 'male']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = data[:int(len(data)*0.8)]\n",
    "test_data = data[int(len(data)*0.8):]\n",
    "\n",
    "model.fit(train_data[features], train_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d69bd",
   "metadata": {},
   "source": [
    "Does the code introduce bias?\n",
    "\n",
    "Yes. The code has potential of introducing algorithmic bias because it is based on the gender of the data and only selects data with gender labeled as \"male\". This could lead to a biased model which is not representative of the true population and could lead to incorrect conclusions. \n",
    "\n",
    "To eliminate the bias, it is important to ensure that the data set used for training is representative of the true population. This could be done by ensuring that the data set is gender balanced and also includes data from other groups such as different age groups, different backgrounds, etc. Additionally, it is important to ensure that the model is evaluated on data that is representative of the true population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bba332e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1298909989.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"import pandas as pd\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Remove rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "data = data[data['income'] >= 50000]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = data[:int(len(data)*0.8)]\n",
    "test_data = data[int(len(data)*0.8):]\n",
    "\n",
    "model.fit(train_data[features], train_data['label'])\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d087a",
   "metadata": {},
   "source": [
    "Does the code introduce bias?\n",
    "\n",
    "Yes. The code has potential to introduce algorithmic bias because it is using income as a feature to train the model. This could lead to the model making decisions that are biased towards people with higher incomes. To eliminate the bias, one suggestion is to use a different set of features that are not related to income. Another suggestion is to use a technique such as cross-validation to ensure that the model is not overfitting to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ffb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "caffe_root = './caffe/' \n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd9995",
   "metadata": {},
   "source": [
    "Does the code introduce bias?\n",
    "\n",
    "No. This code does not introduce any algorithmic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e33971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"gender_net_pretrained='./gender_net.caffemodel'\n",
    "gender_net_model_file='./deploy_gender.prototxt'\n",
    "gender_net = caffe.Classifier(gender_net_model_file, gender_net_pretrained,\n",
    "                       mean=mean,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(256, 256))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52379d3d",
   "metadata": {},
   "source": [
    "Does the code introduce bias?\n",
    "\n",
    "No. The code does not contain any elements that could introduce algorithmic bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}